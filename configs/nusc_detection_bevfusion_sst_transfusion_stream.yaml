voxel_size: [0.15, 0.15, 8]
point_cloud_range: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]


### model 
model:
  type: BEVFusion_SST
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [1, 2, 3]
        with_cp: true
        convert_weights: true
        # init_cfg:
        #   type: Pretrained
        #   checkpoint: pretrained/swint-nuimages-pretrained.pth
      neck:
        type: GeneralizedLSSFPN
        in_channels: [192, 384, 768]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false
      vtransform:
        type: DepthLSSTransform
        in_channels: 256
        out_channels: 80
        image_size: ${image_size}
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        xbound: [-54.0, 54.0, 0.15]
        ybound: [-54.0, 54.0, 0.15]
        zbound: [-10.0, 10.0, 20.0]
        dbound: [1.0, 60.0, 0.5]
        downsample: 1
        
    lidar:
      voxelize:
        point_cloud_range: ${point_cloud_range}
        voxel_size: ${voxel_size}
        max_voxels: [-1, -1]
        max_num_points: -1
        Voxelization: true

      voxel_encoder:
        type: DynamicVFE
        in_channels: 5
        feat_channels: [64, 128]
        with_distance: false
        voxel_size: ${voxel_size}
        with_cluster_center: true
        with_voxel_center: true
        point_cloud_range: ${point_cloud_range}
      
      middle_encoder:
        type: SSTInputLayerV2
        window_shape: [16, 16, 1]
        sparse_shape: [720, 720, 1]
        shuffle_voxels: true
        debug: true
        pos_temperature: 10000
        normalize_pos: false
        mute: true

      backbone:
        type: SSTv2
        d_model: [128, 128, 128, 128, 128, 128, 128, 128]
        nhead: [8, 8, 8, 8, 8, 8, 8, 8]
        num_blocks: 8
        dim_feedforward: [256, 256, 256, 256, 256, 256, 256, 256]
        output_shape: [720, 720]
        num_attached_conv: 3
        conv_in_channel: 128
        conv_out_channel: 128
        checkpoint_blocks: [0, 1, 2, 3]
        debug: true

  fuser:
    type: ConvFuser
    in_channels: [80, 128]
    out_channels: 128

  decoder:
    backbone:
      type: SECOND
      in_channels: 128
      out_channels: [64, 128, 256]
      layer_nums: [3, 5, 5]
      layer_strides: [2, 2, 2]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      conv_cfg:
        type: Conv2d
        bias: false
    neck:
      type: SECONDFPN
      in_channels: [64, 128, 256]
      out_channels: [128, 128, 128]
      upsample_strides: [0.5, 1, 2]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      upsample_cfg:
        type: deconv
        bias: false
      use_conv_for_no_stride: true

  heads:
    map: null
    object:
      type: TransFusionHead
      num_proposals: 200
      auxiliary: true
      in_channels: 384
      hidden_channel: 128
      num_classes: 10
      num_decoder_layers: 1
      num_heads: 8
      nms_kernel_size: 3
      ffn_channel: 256
      dropout: 0.1
      bn_momentum: 0.1
      activation: relu
      train_cfg:
        dataset: nuScenes
        point_cloud_range: ${point_cloud_range}
        grid_size: [720, 720, 1]
        out_size_factor: 4
        voxel_size: ${voxel_size}
        gaussian_overlap: 0.1
        min_radius: 2
        pos_weight: -1
        code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
        assigner:
          type: HungarianAssigner3D
          iou_calculator:
            type: BboxOverlaps3D
            coordinate: lidar
          cls_cost:
            type: FocalLossCost
            gamma: 2.0
            alpha: 0.25
            weight: 0.15
          reg_cost:
            type: BBoxBEVL1Cost
            weight: 0.25
          iou_cost:
            type: IoU3DCost
            weight: 0.25
      test_cfg:
        dataset: nuScenes
        grid_size: [720, 720, 1]
        out_size_factor: 4
        voxel_size: ${voxel_size[:2]}
        pc_range: ${point_cloud_range[:2]}
        nms_type: null
      common_heads:
        center: [2, 2]
        height: [1, 2]
        dim: [3, 2]
        rot: [2, 2]
        vel: [2, 2]
      bbox_coder:
        type: TransFusionBBoxCoder
        pc_range: ${point_cloud_range[:2]}
        post_center_range: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
        score_threshold: 0.0
        out_size_factor: 4
        voxel_size: ${voxel_size[:2]}
        code_size: 10
      loss_cls: 
        type: FocalLoss
        use_sigmoid: true
        gamma: 2.0
        alpha: 0.25
        reduction: mean
        loss_weight: 1.0
      loss_heatmap:
        type: GaussianFocalLoss
        reduction: mean
        loss_weight: 1.0
      loss_bbox:
        type: L1Loss
        reduction: mean
        loss_weight: 0.25
  stream: True

### model 


## training details
num_iters_per_epoch: 32023 # int(28130 // (num_gpus * batch_size) * 4.554)
max_epochs: null
seed: 0
deterministic: false

checkpoint_config:
  interval: 32023 # checkpoint_epoch_interval * num_iters_per_epoch (1*num_iters_per_epoch)

log_config:
  interval: 50
  hooks:
    -
      type: TextLoggerHook
    -
      type: TensorboardLoggerHook

cudnn_benchmark: false
fp16:
  loss_scale: 
    growth_interval: 2000


# Because we use a custom sampler to load data in sequentially during training,
# we can only use IterBasedRunner instead of EpochBasedRunner. To train for a
# fixed # of epochs, we need to know how many iterations are in each epoch. The
# # of iters in each epoch depends on the overall batch size, which is # of 
# GPUs (num_gpus) and batch size per GPU (batch_size). "28130" is # of training
# samples in nuScenes.

runner:
  type: IterBasedRunner
  max_iters: 192138 # num_epochs * num_iters_per_epoch

load_from: null
resume_from: null

# Each nuScenes sequence is ~40 keyframes long. Our training procedure samples
# sequences first, then loads frames from the sampled sequence in order 
# starting from the first frame. This reduces training step-to-step diversity,
# lowering performance. To increase diversity, we split each training sequence
# in half to ~20 keyframes, and sample these shorter sequences during training.
# During testing, we do not do this splitting.

train_sequences_split_num: 2
test_sequences_split_num: 1

data:
  samples_per_gpu: 1
  workers_per_gpu: 2
  train:
    # type: CBGSDataset # no CBGS for streaming pipeline
    # dataset:
    type: ${dataset_type}
    dataset_root: ${dataset_root}
    ann_file: ${dataset_root + "nuscenes_infos_train.pkl"}
    pipeline: ${train_pipeline}
    object_classes: ${object_classes}
    map_classes: ${map_classes}
    modality: ${input_modality}
    test_mode: false
    use_valid_flag: true
    box_type_3d: LiDAR
    use_sequence_group_flag: True
    sequences_split_num: 2
  val:
    type: ${dataset_type}
    dataset_root: ${dataset_root}
    ann_file: ${dataset_root + "nuscenes_infos_val.pkl"}
    pipeline: ${test_pipeline}
    object_classes: ${object_classes}
    map_classes: ${map_classes}
    modality: ${input_modality}
    test_mode: false
    box_type_3d: LiDAR
    use_sequence_group_flag: True
    sequences_split_num: 1
  test:
    type: ${dataset_type}
    dataset_root: ${dataset_root}
    ann_file: ${dataset_root + "nuscenes_infos_val.pkl"}
    # ann_file: ${dataset_root + "nuscenes_infos_test.pkl"}
    pipeline: ${test_pipeline}
    object_classes: ${object_classes}
    map_classes: ${map_classes}
    modality: ${input_modality}
    test_mode: true
    box_type_3d: LiDAR
    use_sequence_group_flag: True
    sequences_split_num: 1

evaluation:
  interval: 192138 # num_epochs * num_iters_per_epoch
  pipeline: ${test_pipeline}
 
optimizer:
  type: AdamW
  lr: 2.0e-4
  weight_decay: 0.01

optimizer_config:
  grad_clip:
    max_norm: 35
    norm_type: 2

lr_config:
  policy: CosineAnnealing
  warmup: linear
  warmup_iters: 500
  warmup_ratio: 0.33333333
  min_lr_ratio: 1.0e-3
  
momentum_config:
  policy: cyclic

## training details


## dataset

dataset_type: NuScenesDataset
dataset_root: data/nuscenes/
gt_paste_stop_epoch: -1
reduce_beams: 32
load_dim: 5
use_dim: 5
load_augmented: null

image_size: [256, 704]

augment2d:
  resize: [[0.38, 0.55], [0.48, 0.48]]
  rotate: [-5.4, 5.4]
  gridmask:
    prob: 0.0
    fixed_prob: true

augment3d:
  scale: [0.9, 1.1]
  rotate: [-0.78539816, 0.78539816]
  translate: 0.5

object_classes:
  - car
  - truck
  - construction_vehicle
  - bus
  - trailer
  - barrier
  - motorcycle
  - bicycle
  - pedestrian
  - traffic_cone

map_classes:
  - drivable_area
  # - drivable_area*
  - ped_crossing
  - walkway
  - stop_line
  - carpark_area
  # - road_divider
  # - lane_divider
  - divider

input_modality:
  use_lidar: true
  use_camera: true
  use_radar: false
  use_map: false
  use_external: false

train_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
  -
    type: LoadPointsFromFile
    coord_type: LIDAR
    load_dim: ${load_dim}
    use_dim: ${use_dim}
    reduce_beams: ${reduce_beams}
    load_augmented: ${load_augmented}
  -
    type: LoadPointsFromMultiSweeps
    sweeps_num: 9
    load_dim: ${load_dim}
    use_dim: ${use_dim}
    reduce_beams: ${reduce_beams}
    pad_empty_sweeps: true
    remove_close: true
    load_augmented: ${load_augmented}
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ObjectPaste
    stop_epoch: ${gt_paste_stop_epoch}
    db_sampler:
      dataset_root: ${dataset_root}
      info_path: ${dataset_root + "nuscenes_dbinfos_train.pkl"}
      rate: 1.0
      prepare: 
        filter_by_difficulty: [-1]
        filter_by_min_points:
          car: 5
          truck: 5
          bus: 5
          trailer: 5
          construction_vehicle: 5
          traffic_cone: 5
          barrier: 5
          motorcycle: 5
          bicycle: 5
          pedestrian: 5
      classes: ${object_classes}
      sample_groups:
        car: 2
        truck: 3
        construction_vehicle: 7
        bus: 4
        trailer: 6
        barrier: 2
        motorcycle: 6
        bicycle: 6
        pedestrian: 2
        traffic_cone: 2
      points_loader:
        type: LoadPointsFromFile
        coord_type: LIDAR
        load_dim: ${load_dim}
        use_dim: ${use_dim}
        reduce_beams: ${reduce_beams}
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[0]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: ${augment2d.rotate}
    rand_flip: true
    is_train: true
  -
    type: GlobalRotScaleTrans
    resize_lim: ${augment3d.scale}
    rot_lim: ${augment3d.rotate}
    trans_lim: ${augment3d.translate}
    is_train: true
  -
    type: LoadBEVSegmentation
    dataset_root: ${dataset_root}
    xbound: [-50.0, 50.0, 0.5]
    ybound: [-50.0, 50.0, 0.5]
    classes: ${map_classes}
  -
    type: RandomFlip3D
  -
    type: PointsRangeFilter
    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectRangeFilter
    point_cloud_range: ${point_cloud_range}
  -
    type: ObjectNameFilter
    classes: ${object_classes}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  - 
    type: GridMask
    use_h: true
    use_w: true
    max_epoch: ${max_epochs}
    rotate: 1
    offset: false
    ratio: 0.5
    mode: 1
    prob: ${augment2d.gridmask.prob}
    fixed_prob: ${augment2d.gridmask.fixed_prob}
  -
    type: PointShuffle
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
      - points
      - gt_bboxes_3d
      - gt_labels_3d
      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix
    meta_lis_keys:
      - filename
      - timestamp
      - ori_shape
      - img_shape
      - lidar2image
      - depth2img
      - cam2img
      - pad_shape
      - scale_factor
      - flip
      - pcd_horizontal_flip
      - pcd_vertical_flip
      - box_mode_3d
      - box_type_3d
      - img_norm_cfg
      - pcd_trans
      - token
      - pcd_scale_factor
      - pcd_rotation
      - lidar_path
      - transformation_3d_flow
      # - weather

test_pipeline:
  -
    type: LoadMultiViewImageFromFiles
    to_float32: true
  -
    type: LoadPointsFromFile
    coord_type: LIDAR
    load_dim: ${load_dim}
    use_dim: ${use_dim}
    reduce_beams: ${reduce_beams}
    load_augmented: ${load_augmented}
  -
    type: LoadPointsFromMultiSweeps
    sweeps_num: 9
    load_dim: ${load_dim}
    use_dim: ${use_dim}
    reduce_beams: ${reduce_beams}
    pad_empty_sweeps: true
    remove_close: true
    load_augmented: ${load_augmented}
  # test
  -
    type: LoadAnnotations3D
    with_bbox_3d: true
    with_label_3d: true
    with_attr_label: False
  -
    type: ImageAug3D
    final_dim: ${image_size}
    resize_lim: ${augment2d.resize[1]}
    bot_pct_lim: [0.0, 0.0]
    rot_lim: [0.0, 0.0]
    rand_flip: false
    is_train: false
  -
    type: GlobalRotScaleTrans
    resize_lim: [1.0, 1.0]
    rot_lim: [0.0, 0.0]
    trans_lim: 0.0
    is_train: false
  -
    type: LoadBEVSegmentation
    dataset_root: ${dataset_root}
    xbound: [-50.0, 50.0, 0.5]
    ybound: [-50.0, 50.0, 0.5]
    classes: ${map_classes}
  -
    type: PointsRangeFilter
    point_cloud_range: ${point_cloud_range}
  -
    type: ImageNormalize
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  -
    type: DefaultFormatBundle3D
    classes: ${object_classes}
  -
    type: Collect3D
    keys:
      - img
      - points
      # test
      - gt_bboxes_3d
      - gt_labels_3d
      - gt_masks_bev
    meta_keys:
      - camera_intrinsics
      - camera2ego
      - lidar2ego
      - lidar2camera
      - camera2lidar
      - lidar2image
      - img_aug_matrix
      - lidar_aug_matrix
    meta_lis_keys:
      - filename
      - timestamp
      - ori_shape
      - img_shape
      - lidar2image
      - depth2img
      - cam2img
      - pad_shape
      - scale_factor
      - flip
      - pcd_horizontal_flip
      - pcd_vertical_flip
      - box_mode_3d
      - box_type_3d
      - img_norm_cfg
      - pcd_trans
      - token
      - pcd_scale_factor
      - pcd_rotation
      - lidar_path
      - transformation_3d_flow
      # - weather

## dataset
